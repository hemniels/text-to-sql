simpleglove.py:
python3 simpleglove.py
Epoch 1/25: 100%|███████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.25s/batch]
Epoch 1/25 - Loss: 12.50948429107666
Epoch 2/25: 100%|███████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.21batch/s]
Epoch 2/25 - Loss: 11.293095111846924
Epoch 3/25: 100%|███████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.21batch/s]
Epoch 3/25 - Loss: 10.006094932556152
Epoch 4/25: 100%|███████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.24batch/s]
Epoch 4/25 - Loss: 8.74601936340332
Epoch 5/25: 100%|███████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.22batch/s]
Epoch 5/25 - Loss: 7.366792440414429
Epoch 23/25: 100%|██████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.26batch/s]
Epoch 23/25 - Loss: 0.450291208922863
Epoch 24/25: 100%|██████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.22batch/s]
Epoch 24/25 - Loss: 0.4342823475599289
Epoch 25/25: 100%|██████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.21batch/s]
Epoch 25/25 - Loss: 0.41182978451251984
Training abgeschlossen.
Predicted SQL Query: the the the the the the the


simpletransformer.py:
python3 simpletransformer.py
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
{'train_runtime': 12.3888, 'train_samples_per_second': 4.036, 'train_steps_per_second': 0.565, 'train_loss': 10.350833347865514, 'epoch': 1.0}
100%|██████████████████████████████████████████████████████████████████████████| 7/7 [00:12<00:00,  1.77s/it]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Aggregate Prediction: SUM
Select Prediction: order_id
Where Prediction: greater than
Generated SQL Query: Show all orders greater than 100 dollar